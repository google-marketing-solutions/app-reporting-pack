{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"App Reporting Pack - Comprehensive reporting for Google Ads App campaigns","text":"<p>App Reporting Pack is a dashboard for Google Ads App campaign data.</p> <p>Crucial information on App campaigns is scattered across various places in Google Ads UI which makes it harder to get insights into how campaign and assets perform.</p> <p>App Reporting Pack fetches all necessary data from Ads API and creates a centralized dashboard showing different aspects of App campaign's performance and settings. All data is stored in BigQuery tables that can be you can use separately.</p>"},{"location":"#features","title":"Features","text":"<p>App Reporting Pack covers the following areas of App campaigns:</p> <ul> <li>Asset level analytics</li> <li>Ad and Asset disapprovals monitoring</li> <li>Bids and budget monitoring</li> <li>Campaign changes monitoring</li> <li>Best practices adoption</li> <li>Store insights (Google Play, App Store)</li> <li>SKAN analytics</li> </ul>"},{"location":"#dashboard-previews","title":"Dashboard previews","text":"<p>Explore the sample dashboard by clicking on the tabs below.</p> assetsbids &amp; budgetshygienechangesdisaprovalsstore insights <p></p> <p></p> <p></p> <p></p> <p></p> <p></p>"},{"location":"#next-steps","title":"Next steps","text":"<p>Install App Reporting Pack</p>"},{"location":"customization/backfill/","title":"Backfill","text":""},{"location":"customization/backfill/#backfill-performance-data","title":"Backfill performance data","text":"<p>If already set up a solution and want your performance tables (<code>asset_performance</code>, <code>asset_converion_split</code>, <code>ad_group_network_split</code>, <code>ios_skan_decoder</code>, <code>geo_performance</code>) to contains historical data from a certain date in the past, please do the following:</p> <ul> <li>In <code>app_reporting_pack.yaml</code> under <code>gaarf &gt; params &gt; macro</code> add <code>initial_load_date: 'YYYY-MM-DD'</code>, where <code>YYYY-MM-DD</code> is the first date you want to have performance data loaded.</li> </ul> <pre><code>gaarf:\n  params:\n    macro:\n      initial_load_date: '2022-01-01'\n      start_date: :YYYYMMDD-30\n      end_date: :YYYYMMDD-1\n</code></pre>"},{"location":"customization/custom-conversions/","title":"Custom conversions","text":""},{"location":"customization/custom-conversions/#providing-custom-conversion-mapping","title":"Providing custom conversion mapping","text":"<p>If you want your tables (<code>asset_performance</code> and <code>ad_group_network_split</code>) to contain custom conversion mapping (i.e. column that contains conversion(s) only with a particular conversion name) you can add the following lines to your <code>app_reporting_pack.yaml</code> in \"conversion_alias: Conversion Name\" format:</p> <pre><code>template:\n  custom_conversions:\n    - conversion_name_1: \"Your Conv','Second Conv\"\n      conversion_name_2: \"My Conv\"\n</code></pre> <p>This will add the following columns to your tables in BigQuery:</p> <ul> <li><code>conversions_conversion_name_1</code></li> <li><code>conversions_value_conversion_name_1</code></li> <li><code>conversions_conversion_name_2</code></li> <li><code>conversions_value_conversion_name_2</code></li> </ul> <p>You can add one or more conversion names to a given conversion alias; in case of several conversion they should be separated with <code>','</code>.</p>"},{"location":"customization/skan/","title":"SKAN Reports in App Reporting Pack","text":"<p>App Reporting Pack (ARP) now includes iOS SKAN Reporting which is a \"one-stop-shop\" solution for iOS performance reporting that integrates &amp; combines clients Google Ads App Campaign &amp; SKAN data. ARP offers 2 SKAN report types:</p> <ul> <li>SKAN postbacks reports: total SKAN postbacks per campaign vs Google Ads installs, SKAN CPI vs. Google Ads CPI, postbacks breakdown by interaction type, NULL/Not NULL conversion value, won/contributed, etc.</li> <li>Custom SKAN conversion reports which require external SKAN schema: number of selected SKAN conversions per campaigns vs. Google Ads biddable conversions, SKAN conversion value vs. Google Ads conversion value, SKAN ROAS vs. Google Ads ROAS, etc.</li> </ul>"},{"location":"customization/skan/#deployment","title":"Deployment","text":"<p>SKAN postbacks reports are available by default. ARP fetches the SKAN data from the Google Ads API and the dashboard provides visualization for the most important reports. In order to activate SKAN conversion reports it\u2019s necessary to provide a SKAN schema, so that ARP would be able to map SKAN conversion values to conversion names, revenue ranges and event count. The easiest way to provide the SKAN schema is to do it while deploying the ARP.</p> <p>During the deployment the wizard will ask you to provide a fully qualified SKAN schema table name. In the example below the table name is <code>arp.skan_schema</code>. It can be located in any dataset in the current GCP project.</p> <p></p> <p>Important</p> <p>When adding schema to an existing <code>app_reporting_pack.yaml</code> add the following fields:</p> <ul> <li>under <code>gaarf-bq &gt; params &gt; macro</code> add <code>skan_schema_input_table: PROJECT.DATASET.TABLE_NAME</code></li> <li>under <code>gaarf-bq &gt; params &gt; template</code> add <code>has_skan: 'true'</code></li> <li>under <code>scripts &gt; skan_mode</code> add <code>skan_mode: table</code> (if <code>skan_mode</code> is set to <code>placeholders</code> replace <code>placeholders</code> with <code>table</code>)</li> </ul> <p>You can refer to config.yaml.template to check config structure.</p> <p>The SKAN table schema is expected to have the following fields: <pre><code>app_id: STRING\nskan_mapped_event: STRING\nskan_conversion_value: INTEGER\nskan_event_count: INTEGER\nskan_event_value_low: FLOAT\nskan_event_value_high: FLOAT\nskan_event_value_mean: FLOAT\n</code></pre></p> <p>The table schema is similar to the format used by trackers like AppsFlyer, Adjust and others. For example, skan_event_value_low and skan_event_value_high match AppsFlyer\u2019s min_revenue and max_revenue columns from the CSV SKAN schema; skan_event_value_mean is mean value of min and max revenue; skan_event_count is the same as event_counter column.</p> <p>You can create a SKAN schema table in BQ and copy the data manually, but the most convenient way is to create an external BQ table linked to Google Spreadsheet document with the schema. Then it will be much easier to keep the schema up to date as the BQ table will be automatically updated when you make changes in the Google Sheet. To do so create a Google spreadsheet with the required columns and share the spreadsheet with a service account which has access to BQ (e.g. with Compute Engine default service account). After that create a table as shown below.</p> <p></p>"},{"location":"customization/skan/#reports-overview","title":"Reports Overview","text":""},{"location":"customization/skan/#skan-postbacks-sheet","title":"SKAN Postbacks sheet","text":"<ol> <li>Postback breakdowns and filters. The donut charts are interactive, click on an area to apply filters to the whole sheet.</li> </ol> <ol> <li>SKAN Installs (postbacks), Google Ads installs and other metrics per campaign</li> </ol> <ol> <li>SKAN Coverage charts. SKAN coverage = SKAN Installs/GAds Installs</li> </ol> <ol> <li>SKAN CPI (Cost per Postback) vs Google Ad CPI</li> </ol> <ol> <li>SKAN Conversion Rate (SKAN Postbacks/clicks) vs Google Ads Conversion Rate &amp; CTR</li> </ol>"},{"location":"customization/skan/#skan-in-app-sheet-postbacks-with-cv-0","title":"SKAN In-app sheet (postbacks with CV &gt; 0)","text":"<ol> <li>Postback breakdowns and filters. There is an additional SKAN Event dropdown filter where you can select custom events you want to filter on. The donut charts are interactive, click on an area to apply filters to the whole sheet.</li> </ol> <p>IMPORTANT: it\u2019s necessary to manually select biddable SKAN conversions in the filter for the charts on this sheet to make sense.</p> <ol> <li>Generic in-app conversions metrics. Count of selected SKAN conversions vs biddable GAds conversions, CPA for the selected SKAN conversions vs GAds CPA, SKAN conversion rate (SKAN conversions/Clicks) vs GAds conversion rate.</li> </ol> <p></p> <ol> <li>tCPA campaigns metrics. Target CPA vs Google Ads actual CPA vs SKAN CPA over time. SKAN InApp Coverage (SKAN conversions/GAds biddable conversions), SKAN Install Coverage (SKAN Installs/GAds Installs), SKAN Conversions and tCPA Bid Utilization (Actual GAds CPA/Target CPA).</li> </ol> <p></p> <ol> <li>tROAS campaigns metrics. SKAN Conversion values vs GAds conversion value. By default it shows SKAN Value Mean, but you can also show Low and High from the optional metrics. Google Ads ROAS vs SKAN ROAS vs Target ROAS. By default SKAN ROAS is calculated using SKAN Value Mean, but you can also show Low and High from the optional metrics. tROAS and SKAN Bid Utilization (actual GAds and SKAN ROAS/Target ROAS) and In-app SKAN Coverage (SKAN InApps/GAds biddable in-apps).</li> </ol> <p></p>"},{"location":"customization/video-orientation/","title":"How to get video orientation for assets","text":"<p>Video orientation data is not readily available in Google Ads API but can be fetched with YouTube Data API.</p> <p>In order to get this data for App Reporting Pack do the following:</p> <ul> <li>Enable YouTube Data API</li> <li>API key to access to YouTube Data API.</li> <li> <p>Once you created API key export it as an environmental variable</p> <pre><code>export GOOGLE_API_KEY=&lt;YOUR_API_KEY_HERE&gt;\n</code></pre> </li> </ul> <p>Important</p> <p>During Google Cloud installation of App Reporting Pack this API is created automatically.</p>"},{"location":"dashboard/replication/","title":"Dashboard","text":"<p>Important</p> <p>Join <code>app-reporting-pack-readers-external</code> Google group  to get access to the dashboard template.</p> <p>If you install ARP in Google Cloud then in most cases you don't need the following procedure. Otherwise use the following command to clone the ARP dashboard.</p> <p>Once queries ran successfully you can proceed with dashboard replication.</p> <p>Run the following command in the terminal to get a link for cloning the dashboard:</p> <pre><code>bash ./app/scripts/create_dashboard.sh -c app/app_reporting_pack.yaml -L\n</code></pre> <p>If you're running on a local machine you can omit <code>-L</code> flag and then the link will be opened in the browser.</p> <p>Important</p> <p>After the dashboard is created you need to enable image previews, read details on how it can be done here.</p>"},{"location":"dashboard/replication/#automatic-creation","title":"Automatic creation","text":"<ol> <li> <p>Go to <code>scripts</code> folder</p> </li> <li> <p>Run <pre><code>./create_dashboard.sh -p &lt;YOUR_BQ_PROJECT_ID&gt; -d &lt;YOUR_BQ_DATASET_ID&gt;\n</code></pre></p> </li> </ol> <p>where YOUR_BQ_PROJECT_ID and YOUR_BQ_DATASET_ID are the names of the BQ project and dataset that contains App Reporting Pack data.\\</p> <p>You can provide optional arguments to the script:</p> <ul> <li><code>--name=\"&lt;YOUR_NAME_FOR_THE_DASHBOARD&gt;\"</code> - where YOUR_NAME_FOR_THE_DASHBOARD is the name of the generated dashboard.</li> <li><code>--lite</code> - if you want to install simplified version of App Reporting Pack</li> </ul>"},{"location":"dashboard/replication/#manual-creation","title":"Manual creation","text":"<p>The process of manual replication consists of two steps:</p> <ul> <li>Replication of datasources</li> <li>Replication of dashboard</li> </ul>"},{"location":"dashboard/replication/#replicate-datasources","title":"Replicate datasources","text":"<p>Before replicating the dashboard you need to make copies of datasources that power up the dashboard. Replication of the datasources is important since they contains multiple calculated metrics which could be problematic to create from scratch.</p> <p>Make a copy of each of the following datasources used in the template dashboard.</p> <ul> <li>asset_performance</li> <li>approval_statuses</li> <li>creative_excellence</li> <li>change_history</li> <li>performance_grouping</li> <li>ad_group_network_split</li> </ul> <p>In order to replicate a datasource, please do the following:</p> <ul> <li> <p>Click on the datasource link above.</p> </li> <li> <p>Click on Make a copy of this datasource</p> </li> </ul> <p></p> <ul> <li>Confirm copying by clicking Copy Data Source</li> </ul> <p></p> <ul> <li> <p>Select MY PROJECTS and either pick a project or enter project id manually (this should be the project where App Reporting Pack tables are located)</p> </li> <li> <p>In Dataset select a BQ dataset where App Reporting Pack tables are located </p> </li> <li> <p>Select a table from the dataset which the name similar to Data Source name (i.e., if Data Source is called Assets look for the table which is called assets)</p> </li> </ul> <p></p> <ul> <li>Confirm copying by clicking RECONNECT button.</li> </ul> <p></p> <p>Important</p> <p>Don\u2019t forget to rename the datasource so you can find it easily. I.e. such name as Copy of BQ Template ARP Assets is a bit mouthful, you can name it simply ARP Assets or YOUR-COMPANY-NAME ARP Assets.</p> <ul> <li>Repeat the steps above for all the datasources.</li> </ul> <p>Now that you\u2019ve copied each of the datasources, make a copy of the dashboard and replace each of the template\u2019s datasources with the corresponding datasource you copied.</p>"},{"location":"dashboard/replication/#replication-of-the-dashboard","title":"Replication of the dashboard","text":"<p>Important</p> <p>Ensure that ALL datasources are created before proceeding to replication of the dashboard.</p> <p>Here you can access the template version of the dashboard.</p> <p>Note</p> <p>Lite version of the dashboard is also available.</p> <p>In order to replicate dashboard please do the following:</p> <ul> <li>make a copy of the dashboard by clicking on More options - Make a copy.</li> </ul> <p></p> <ul> <li>In Copy this report window map original datasources to the ones you created in the previous step.</li> </ul> <p></p> <p>Once all template datasources are replaced with new ones, click Copy Report and enjoy your new shiny App Reporting Pack!</p>"},{"location":"dashboard/replication/#post-setup","title":"Post setup","text":""},{"location":"dashboard/replication/#grant-access","title":"Grant access","text":"<p>Once the dashboard is saved it's important that all its users have access to underlying BigQuery dataset.</p> <p>If it's not possible, you'll need to change data credentials for all datasources from Viewer to Owner (see details here).</p>"},{"location":"dashboard/replication/#enable-image-previews","title":"Enable image previews","text":"<p>After the dashboard is created all rendered images are disabled. To activate them follow the steps below:</p> <ul> <li>Click on Edit button</li> <li>Go to Resource  and select the asset_performance_copy datasource.</li> <li>Search for <code>asset_preview</code> field</li> <li>Click on three docs and select Show to enable preview</li> </ul> <p></p>"},{"location":"installation/airflow/","title":"Airflow","text":"<p>Running App Reporting Pack queries in Apache Airflow is easy. You'll need to provide three arguments for running <code>DockerOperator</code> inside your DAG:</p> <ul> <li><code>/path/to/google-ads.yaml</code> - absolute path to <code>google-ads.yaml</code> file (can be remote)</li> <li><code>service_account.json</code> - absolute path to service account json file</li> <li><code>/path/to/app_reporting_pack.yaml</code> - absolute path to YAML config.</li> </ul>"},{"location":"installation/airflow/#example-dags","title":"Example DAGs","text":""},{"location":"installation/airflow/#getting-configuration-files-locally","title":"Getting configuration files locally","text":"<p>Don't forget to change <code>/path/to/google-ads.yaml</code>, <code>path/to/service_account.json</code> and <code>path/to/app_reporting_pack.yaml</code> with valid paths.</p> <pre><code>from airflow import DAG\nfrom datetime import datetime, timedelta\nfrom airflow.providers.docker.operators.docker import DockerOperator\nfrom docker.types import Mount\n\n\ndefault_args = {\n    'description'           : 'https://github.com/google/app-reporting-pack',\n    'depend_on_past'        : False,\n    'start_date'            : datetime(2023, 3, 1),\n    'email_on_failure'      : False,\n    'email_on_retry'        : False,\n    'retries'               : 1,\n    'retry_delay'           : timedelta(minutes=5)\n}\nwith DAG(\n    'app_reporting_pack_local',\n    default_args=default_args,\n    schedule_interval=\"* 0 * * *\",\n    catchup=False) as dag:\n    app_reporting_pack = DockerOperator(\n        task_id='app_reporting_pack_docker',\n        image='ghcr.io/google-marketing-solutions/app-reporting-pack:latest',\n        api_version='auto',\n        auto_remove=True,\n        command=[\n            \"-g\", \"/google-ads.yaml\",\n            \"-c\", \"/app_reporting_pack.yaml\",\n        ],\n        docker_url=\"unix://var/run/docker.sock\",\n        mounts=[\n            Mount(\n                source=\"/path/to/service_account.json\",\n                target=\"/app/service_account.json\",\n                type=\"bind\"),\n            Mount(\n                source=\"/path/to/google-ads.yaml\",\n                target=\"/google-ads.yaml\",\n                type=\"bind\"),\n            Mount(\n                source=\"/path/to/app_reporting_pack.yaml\",\n                target=\"/app_reporting_pack.yaml\",\n                type=\"bind\")\n        ]\n    )\n    app_reporting_pack\n</code></pre>"},{"location":"installation/airflow/#getting-configuration-files-from-google-cloud-storage","title":"Getting configuration files from Google Cloud Storage","text":"<p>Important</p> <p>Don't forget to change <code>gs://path/to/google-ads.yaml</code>, <code>path/to/service_account.json</code> and <code>path/to/app_reporting_pack.yaml</code> with valid paths.</p> <pre><code>from airflow import DAG\nfrom datetime import datetime, timedelta\nfrom airflow.providers.docker.operators.docker import DockerOperator\nfrom docker.types import Mount\n\n\ndefault_args = {\n    'description'           : 'https://github.com/google/app-reporting-pack',\n    'depend_on_past'        : False,\n    'start_date'            : datetime(2023, 3, 1),\n    'email_on_failure'      : False,\n    'email_on_retry'        : False,\n    'retries'               : 1,\n    'retry_delay'           : timedelta(minutes=5)\n}\nwith DAG(\n    'app_reporting_pack_remote',\n    default_args=default_args,\n    schedule_interval=\"* 0 * * *\",\n    catchup=False) as dag:\n    app_reporting_pack = DockerOperator(\n        task_id='app_reporting_pack_docker',\n        image='ghcr.io/google-marketing-solutions/app-reporting-pack:latest',\n        api_version='auto',\n        auto_remove=True,\n        environment={\n            \"GOOGLE_CLOUD_PROJECT\": &lt;YOUR_CLOUD_PROJECT&gt;\n        },\n        command=[\n            \"-g\", \"gs://path/to/google-ads.yaml\",\n            \"-c\", \"/app_reporting_pack.yaml\",\n        ],\n        docker_url=\"unix://var/run/docker.sock\",\n        mounts=[\n            Mount(\n                source=\"/path/to/service_account.json\",\n                target=\"/app/service_account.json\",\n                type=\"bind\"),\n            Mount(\n                source=\"/path/to/app_reporting_pack.yaml\",\n                target=\"/app_reporting_pack.yaml\",\n                type=\"bind\")\n    )\n    app_reporting_pack\n</code></pre>"},{"location":"installation/docker/","title":"Docker","text":""},{"location":"installation/docker/#prerequisites","title":"Prerequisites","text":"<ul> <li>Google Ads API access and google-ads.yaml file - follow documentation on API authentication.</li> <li>Python 3.9+</li> <li> <p>Service account created and service account key downloaded in order to write data to BigQuery.</p> </li> <li> <p>Once you downloaded service account key export it as an environmental variable</p> <pre><code>export GOOGLE_APPLICATION_CREDENTIALS=path/to/service_account.json\n</code></pre> </li> <li> <p>If authenticating via service account is not possible you can authenticate with the following command:     <pre><code>gcloud auth application-default login\n</code></pre></p> </li> </ul>"},{"location":"installation/docker/#running-in-docker","title":"Running in Docker","text":"<p>You can run App Reporting Pack queries inside a Docker container.</p> <pre><code>docker run \\\n    -v /path/to/google-ads.yaml:/google-ads.yaml \\\n    -v /path/to/service_account.json:/app/service_account.json \\\n    -v /path/to/app_reporting_pack.yaml:/app_reporting_pack.yaml \\\n    ghcr.io/google-marketing-solutions/app-reporting-pack \\\n    -g google-ads.yaml -c app_reporting_pack.yaml --legacy --backfill\n</code></pre> <p>Important</p> <p>Don't forget to change /path/to/google-ads.yaml and /path/to/service_account.json with valid paths.</p> <p>You can provide configs as remote (for example Google Cloud Storage). In that case you don't need to mount <code>google-ads.yaml</code> and <code>app_reporting_pack.yaml</code> configs into the container:</p> <pre><code>docker run \\\n    -v /path/to/service_account.json:/app/service_account.json \\\n    ghcr.io/google-marketing-solutions/app-reporting-pack \\\n    -g gs://project_name/google-ads.yaml \\\n    -c gs://project_name/app_reporting_pack.yaml \\\n    --legacy --backfill\n</code></pre>"},{"location":"installation/gcp/","title":"Google Cloud","text":""},{"location":"installation/gcp/#prerequisites","title":"Prerequisites","text":"<ol> <li> <p>A Google Ads Developer token</p> </li> <li> <p>A GCP project with billing account attached; a person responsible for deployment of App Reporting Pack should have OWNER or ADMIN permissions to the project.</p> </li> <li> <p>Credentials for Google Ads API access - <code>google-ads.yaml</code>.    See details here</p> </li> </ol>"},{"location":"installation/gcp/#installation","title":"Installation","text":"<ol> <li> <p>Clone repo in Cloud Shell or on your local machine (we assume Linux with <code>gcloud</code> CLI installed): <pre><code>git clone https://github.com/google-marketing-solutions/app-reporting-pack\n</code></pre></p> </li> <li> <p>Go to the repo folder: <code>cd app-reporting-pack/</code></p> </li> <li> <p>Optionally put your <code>google-ads.yaml</code> there or be ready to provide all Ads API credentials</p> </li> <li> <p>Optionally adjust settings in <code>gcp/settings.ini</code></p> </li> <li> <p>Run installation:</p> </li> </ol> <pre><code>./gcp/install.sh\n</code></pre>"},{"location":"installation/gcp/#updating","title":"Updating","text":"<p>To update the existing version of App Reporting Pack</p> <ol> <li>Clone repo in Cloud Shell or on your local machine (we assume Linux with <code>gcloud</code> CLI installed): <pre><code>git clone https://github.com/google-marketing-solutions/app-reporting-pack\n</code></pre></li> </ol> <p>Note</p> <p>If you already have <code>app-reporting-pack</code> repository clone, run the following</p> <pre><code>git stash\ngit pull\n</code></pre> <ol> <li> <p>Go to the repo folder: <code>cd app-reporting-pack/</code></p> </li> <li> <p>Run installation:</p> </li> </ol> <pre><code>./gcp/upgrade.sh\n</code></pre>"},{"location":"installation/gcp/#troubleshooting","title":"Troubleshooting","text":"<p>The most important thing to understand - you should use Cloud Logging to diagnose and track execution progress of ARP VMs. In the end of execution VMs are deleted. If you see an existing VM left from execution it's a signal of something wrong happened.</p>"},{"location":"installation/gcp/#no-public-ip","title":"No public IP","text":"<p>By default virtual machines created by the Cloud Function have no public IP address associated. It's done to work around possible issues with policies in GCP projects forbid assigning public IPs for VMs.</p> <p>It's managed by the <code>no-public-ip</code> option in the <code>settings.ini</code> in <code>[compute]</code> section. By default it's <code>true</code> so VMs created without public IPs. As they still need to get access to the Artifact Repository for downloading the Docker image, the option 'Private Google Access' is enabled for the default subnetwork. It's done by <code>setup.sh</code>. But VMs created by the CF get into another subnetwork in your environment them you have to enable the option manually.</p> <p>To enable the option follow these steps:</p> <ul> <li>open GCE virtual machines</li> <li>choose a ARP VM, it will have name like arp-vm-xxxxxxxxxxxxx</li> <li>click on its network interface (next to its internal IP - usually \"nic0\")</li> <li>in the first list \"Network interface details\" click on \"default\" subnetwork for the interface</li> <li>click Edit and enable 'Private Google Access' option, save</li> </ul>"},{"location":"installation/gcp/#docker-image-failed-to-build","title":"Docker image failed to build","text":"<p>If you're getting an error at the creating Docker repository step:</p> <pre><code>ERROR: (gcloud.artifacts.repositories.create) INVALID_ARGUMENT: Maven config is not supported for format \"DOCKER\"\n- '@type': type.googleapis.com/google.rpc.DebugInfo\n  detail: '[ORIGINAL ERROR] generic::invalid_argument: Maven config is not supported\n    for format \"DOCKER\" [google.rpc.error_details_ext] { code: 3 message: \"Maven config\n    is not supported for format \\\"DOCKER\\\"\" }'\n</code></pre> <p>Please update your Cloud SDK CLI by running <code>gcloud components update</code></p>"},{"location":"installation/gcp/#no-google-cloud-storage-public-access","title":"No Google Cloud Storage public access","text":"<p>If your GCP project has a policy to prevent public access to GCS then during installation <code>setup.sh</code> won't be able to deploy a webpage (index.html) for waiting for the completion from which you could clone the dashboard. In that case you will have to replicate the dashboard manually - see Dashboard Replication.</p>"},{"location":"installation/gcp/#vm-ran-but-did-nothing-and-was-not-deleted","title":"VM ran but did nothing and was not deleted","text":"<p>Normally the Cloud Function <code>create-vm</code> creates a VM which runs ARP as Docker container. Though for this to work the VM should download an ARP image from Artifact Repository. If your GCP project forbids for GCE VMs to have public IPs then by default they don't have access to any Google Cloud services. To work around this issue you need to enable 'Private Google Access' option for the default subnetwork. It should be enabled automatically by default but worth checking - go to your VM's settings and check whcih subnetwork it got into and then check the setting 'Private Google Access' is enabled for that subnetwork.</p> <p>Please see No public IP for details.</p> <p>You can safely delete all ARP virtual machines and rerun Scheduler job manually. On a next run a VM should properly start with Google network access and download ARP image. In the end the VM will be removed.</p>"},{"location":"installation/local/","title":"Local","text":""},{"location":"installation/local/#prerequisites","title":"Prerequisites","text":"<ul> <li>Google Ads API access and google-ads.yaml file - follow documentation on API authentication.</li> <li>Python 3.9+</li> <li> <p>Service account created and service account key downloaded in order to write data to BigQuery.</p> </li> <li> <p>Once you downloaded service account key export it as an environmental variable</p> <pre><code>export GOOGLE_APPLICATION_CREDENTIALS=path/to/service_account.json\n</code></pre> </li> <li> <p>If authenticating via service account is not possible you can authenticate with the following command:     <pre><code>gcloud auth application-default login\n</code></pre></p> </li> </ul>"},{"location":"installation/local/#installation","title":"Installation","text":"<p>In order to run App Reporting Pack locally please follow the steps outlined below:</p> <ol> <li>clone this repository   <pre><code>git clone https://github.com/google-marketing-solutions/app-reporting-pack\ncd app-reporting-pack\n</code></pre></li> <li>(Recommended) configure virtual environment if you starting testing the solution:   <pre><code>python -m venv app-reporting-pack\nsource app-reporting-pack/bin/activate\n</code></pre></li> <li> <p>install dependencies:   <code>pip install -r --require-hashes app/requirements.txt</code></p> </li> <li> <p>Run <code>run-local.sh</code> script in a terminal to generate all necessary tables for App Reporting Pack:</p> </li> </ol> <pre><code>./app/run-local.sh\n</code></pre> <p>It will guide you through a series of questions to get all necessary parameters to run the scripts:</p> <ul> <li><code>account_id</code> - id of Google Ads MCC account (no dashes, 111111111 format)</li> <li><code>BigQuery project_id</code> - id of BigQuery project where script will store the data (i.e. <code>my_project</code>)</li> <li><code>BigQuery dataset</code> - id of BigQuery dataset where script will store the data (i.e. <code>my_dataset</code>)</li> <li><code>Reporting window</code> - Number of days (i.e. <code>90</code>) for performance data fetching.</li> <li><code>end date</code> - last date from which you want to get performance data (i.e., <code>2022-12-31</code>). Relative dates are supported see more.</li> <li><code>Ads config</code> - path to <code>google-ads.yaml</code> file. After the initial run of <code>run-local.sh</code> command it will generate <code>app_reporting_pack.yaml</code> config file with all necessary information to be used for future runs. When you run <code>./run-local.sh</code> next time it will automatically pick up the created configuration.</li> </ul>"},{"location":"installation/local/#schedule-running-run-localsh-as-a-cronjob","title":"Schedule running <code>run-local.sh</code> as a cronjob","text":"<p>When running <code>run-local.sh</code> scripts you can specify two options which are useful when running queries periodically (i.e. as a cron job):</p> <ul> <li><code>-c &lt;config&gt;</code>- path to <code>app_reporting_pack.yaml</code> config file. Comes handy when you have multiple config files or the configuration is located outside of current folder.</li> <li><code>-q</code> - skips all confirmation prompts and starts running scripts based on config file.</li> </ul> <p><code>run-local.sh</code> support <code>--legacy</code> command line flag which is used to generate dashboard in the format compatible with existing dashboard. If you're migrating existing datasources <code>--legacy</code> option might be extremely handy.</p> <p>If you installed all requirements in a virtual environment you can use the trick below to run the proper cronjob:</p> <pre><code>* 1 * * * /usr/bin/env bash -c \"source /path/to/your/venv/bin/activate \\\n  &amp;&amp; bash /path/to/app-reporting-pack/app/run-local.sh \\\n  -c /path/to/app_reporting_pack.yaml -g /path/to/google-ads.yaml -q\"\n</code></pre> <p>This command will execute App Reporting Pack queries every day at 1 AM.</p>"},{"location":"installation/overview/","title":"Overview","text":"<p>There are several ways to install the application.</p> <ul> <li> Google Cloud Recommended</li> <li> Docker</li> <li> Airflow</li> <li> Local</li> </ul>"},{"location":"installation/overview/#step-by-step-deployment","title":"Step-by-step deployment","text":"<p>Regardless of the method chosen you'll need to provide some information to configure App Reporting Pack.</p> <ol> <li>Validate that the dependencies are met.     </li> </ol>"},{"location":"installation/overview/#default-parameters","title":"Default parameters","text":"<p>The script will offer to deploy ARP with default settings.     </p> <ul> <li>account_id - Google Ads account or MCC Id provided in the google-ads.yaml.</li> <li>BigQuery project_id - current GCP project where the user is authenticated.</li> <li>BigQuery dataset - dataset in the current project where data will be stored.</li> <li>Reporting window - how much data will be daily extracted from Google Ads. You won't be able to see more data in your dashboard outside of reporting window.</li> <li>Ads config path - path to the provided google-ads.yaml.</li> <li>Cohorts - a list of offsets in days since the attributed interaction. ARP calculates how many conversions occur on 1st, 2nd, 5th day after an interaction with an ad.</li> </ul> <p>Note</p> <p>You can either accept the default setting or press N to configure each parameter. But even if accepts the default settings you still will be asked additional question to configure optional SKAN schema for SKAN in-app events decoding.</p>"},{"location":"installation/overview/#custom-parameters","title":"Custom parameters","text":"<p>If you opted out from the default settings you will be asked to set it up one by one. In most cases a default value will be offered (shown in parenthesis). In order to accept the default value a user can press Enter without typing anything.</p> <p></p>"},{"location":"installation/overview/#cohorts","title":"Cohorts","text":"<p>Enter a comma separated list of numbers. If non-default values are used then some changes in the Looker Studio dashboard may be required, as the reports are configured to be used with the default cohorts.</p>"},{"location":"installation/overview/#skan-schema","title":"SKAN schema","text":"<p>SKAN Schema is optional, it\u2019s required if you want to have decoded SKAN in-app conversions in the report.</p> <p>The schema is a BigQuery table in any dataset, not necessarily the dataset with the ARP data. The name of the table can also be arbitrary. After execution of the script the schema will be copied to the ARP dataset.</p> <p>The script expects the schema as a fully qualified table name like <code>project.dataset.table_name</code>.</p> <p></p> <p>For more information on the SKAN schema set up see SKAN ARP guide.</p>"},{"location":"installation/overview/#advanced-setup","title":"Advanced setup","text":"<p>ARP can be configured manually by editing the config.yaml file. Usually config.yaml is created by the deployment script based on the answers on the questions, but it\u2019s also possible to create it from scratch or modify the existing file.</p> <p>Sample <code>config.yaml</code>:</p> <pre><code>gaarf:\n  output: bq\n  bq:\n    project: YOUR-BQ-PROJECT\n    dataset: arp\n  api_version: '21'\n  account:\n  - 'YOUR_MCC_ID'\n  customer_ids_query: SELECT customer.id FROM campaign WHERE campaign.advertising_channel_type\n    = \"MULTI_CHANNEL\"\n  params:\n    macro:\n      start_date: :YYYYMMDD-91\n      end_date: :YYYYMMDD-1\ngaarf-bq:\n  project: YOUR-BQ-PROJECT\n  params:\n    macro:\n      bq_dataset: arp\n      target_dataset: arp_output\n      legacy_dataset: arp_legacy\n      skan_schema_input_table: YOUR_PROJECT.YOUR_DATASET.YOUR_SKAN_SCHEMA_TABLE\n    template:\n      cohort_days: 1,3,5,7,14,30\n      has_skan: 'true'\nscripts:\n  skan_mode:\n    mode: placeholders\nbackfill: true\nincremental: false\nlegacy: true\n</code></pre> <p>Important parameters:</p> <ul> <li>api_version - current Google Ads API version. It\u2019s recommended to keep it up to date and use the most recent version. Please follow ARP updates.</li> <li>start_date - beginning of the reporting window. Usually it\u2019s a macro :YYYYMMDD-N, where N is the length of the reporting window. It\u2019s not recommended to to change start date as it may cause data disruption</li> <li>end_date - end of the reporting window. By default it\u2019s the previous day YYYYMMDD-1</li> <li>dataset - name of the main dataset. ARP stores all the intermediate tables there</li> <li>target_dataset - name of the dataset where the output tables are stored</li> <li>legacy_dataset - name of the dataset for views for backward compatibility with older versions of ARP</li> <li>skan_schema_input_table - source SKAN schema name. If the schema wasn\u2019t provided during the initial deployment, it can be set here</li> <li>skan_mode.mode - should be \u201ctable\u201d if skan_schema_input_table is provided. Otherwise it\u2019s \u201cplaceholder\u201d</li> <li>has_skan - whether to extract SKAN reports. By default it\u2019s true</li> </ul>"}]}